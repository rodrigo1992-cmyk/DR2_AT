{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Rodrigo_Mesquita_DR2_AT**\n",
    "## **Coleta de Dados com Python via APIs e WebScraping [24E3_2]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook disponível em: https://github.com/rodrigo1992-cmyk/DR2_AT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Docs e Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exercício 1: \n",
    "Baixe seu perfil no Linkedin em PDF e utilize o PyPDF2 para construir uma função que retorne a string do texto completo do documento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "Contato\n",
      "Rio de Janeiro, RJ - Brasil\n",
      "(21) 98113-5927  (Mobile)\n",
      "rodrigomesquita0@gmail.com\n",
      "www.linkedin.com/in/rodrigo-\n",
      "mesquita-6120b193  (LinkedIn)\n",
      "Principais competências\n",
      "Inglês\n",
      "Scikit-Learn\n",
      "SciPy\n",
      "Languages\n",
      "Inglês  (Professional Working)Rodrigo Mesquita\n",
      "Tech Lead | Espec. Sr. em Analytics & BI | Cientista de Dados | MBA\n",
      "Eng. de Softwares | MBA Gestão Empresarial\n",
      "Rio de Janeiro, Rio de Janeiro, Brasil\n",
      "Resumo\n",
      "Profissional de Dados focado em empresas de tecnologia e\n",
      "consultoria, com sólida experiência em gerenciamento de projetos\n",
      "de Business Intelligence e automação de processos analíticos.\n",
      "Lider de uma célula de análise de dados avançadas na TIM\n",
      "Brasil, buscando a democratização do acesso a dados através\n",
      "da capacitação e orientação das áreas de negócio, auxiliando a\n",
      "empresa a construir uma forte cultura data-driven.\n",
      "Cursando segunda graduação, em Data Science pelo Instituto\n",
      "Infnet, além de possuir MBA em Engenharia de Software pela UFRJ,\n",
      "MBA em Gestão Empresarial pela Universidade Cândido Mendes,\n",
      "especialização em Design de Interação pela Faculdade Senac-RJ e\n",
      "graduação em Design pela Universidade do Grande Rio.\n",
      "Experiência\n",
      "TIM Brasil\n",
      "6 anos 10 meses\n",
      "Especialista Sr em Analytics e BI [Tech Lead]\n",
      "agosto de 2023 - Present  (1 ano 2 meses)\n",
      "Especialista em Analytics e BI\n",
      "setembro de 2020 - agosto de 2023  (3 anos)\n",
      "Rio de Janeiro, Brasil\n",
      "》Start-up de um C4E, responsável por promover a evolução dos processos\n",
      "informacionais dentro da diretoria de engenharia, democratizando a acesso\n",
      "a dados de todo CTO e atuando como facilitador em projetos de dados, BI e\n",
      "Machine Learning.\n",
      "》Gestão de equipes de Analytics e BI, com adequação dos times à utilização\n",
      "do framework Scrum. Treinamento e orientação contínua de stakeholders\n",
      "e pontos focais, de forma a promover a democratização e uso sustentável\n",
      "  Page 1 of 4\n",
      "   \n",
      "dos dados e ferramentas disponíveis, buscando a evolução para uma cultura\n",
      "Data-Driven.\n",
      "》Construção de Dashboards em Power BI para suporte aos comitês C-level.\n",
      "Busca contínua por insights oriundos do tratamento ou cruzamento das bases\n",
      "de dados dos sistemas, a fim de implementar novos KPI's de performance\n",
      "operacional e financeira, ou otimizar a obtenção dos KPIs já existentes.\n",
      "》Construção e governança de pipelines de dados, de forma a garantir\n",
      "a consistência dos processos de coleta, manipulação e armazenamento,\n",
      "objetivando mitigar os silos de informação existentes na companhia.\n",
      "Consultor sênior\n",
      "maio de 2019 - agosto de 2020  (1 ano 4 meses)\n",
      "Rio de Janeiro, Brasil\n",
      "》Responsável pela gestão de duas fábricas de softwares terceirizadas\n",
      "que realizavam a manutenção de sistemas de controle financeiro e Supply\n",
      "Chain. De forma complementar, também atuando na criação de soluções\n",
      "analíticas de Lifecycle de contratos, gestão financeira e controle de estoque\n",
      "de materiais, utilizando Sharepoint, Knime, SGBD Oracle e Microsoft Power\n",
      "BI.\n",
      "Consultor de gestão de projetos\n",
      "dezembro de 2017 - abril de 2019  (1 ano 5 meses)\n",
      "Rio de Janeiro, Brasil\n",
      "》Responsável pela gestão de duas fábricas de softwares terceirizadas\n",
      "que realizavam a manutenção de sistemas de controle financeiro e Supply\n",
      "Chain. De forma complementar, também atuando na criação de soluções\n",
      "analíticas de Lifecycle de contratos, gestão financeira e controle de estoque\n",
      "de materiais, utilizando Sharepoint, Knime, SGBD Oracle e Microsoft Power\n",
      "BI.\n",
      "Gluck Serviços e Comercio de Informática \n",
      "Consultor de gestão de projetos\n",
      "março de 2016 - maio de 2017  (1 ano 3 meses)\n",
      "Rio de Janeiro e Região, Brasil\n",
      "》Atuação como Product Owner, realizando a mediação de demandas entre\n",
      "os clientes e a fábrica de softwares, assim como a escrita das user stories,\n",
      "priorização de backlog, acompanhamento do desenvolvimento, gestão de\n",
      "prazos e a validação das entregas.\n",
      "  Page 2 of 4\n",
      "   \n",
      "everis\n",
      "Analista de suporte a projetos\n",
      "junho de 2014 - janeiro de 2015  (8 meses)\n",
      "Rio de Janeiro e Região, Brasil\n",
      "》Prestação de serviços de consultoria com foco na melhoria contínua de\n",
      "sistemas, atuando na criação de visões analíticas, Data Quality e integração\n",
      "de bases de dados. Interação com a fábrica de softwares para criação de\n",
      "demandas para melhoria de usabilidade, otimização de processos, correções\n",
      "nas bases de dados e criação de novas consultas (Queries).\n",
      "Between do Brasil\n",
      "Analista de suporte a projetos\n",
      "junho de 2012 - junho de 2014  (2 anos 1 mês)\n",
      "》Prestação de serviços de consultoria com foco na especificação de\n",
      "requisitos para desenvolvimento de sistemas WFM, utilizados na gestão de\n",
      "processos e contratos terceirizados. Atuação como Product Owner, realizando\n",
      "a interação entre o contratante e a empresa de desenvolvimento de softwares.\n",
      "GVT\n",
      "Técnico de telecomunicações\n",
      "abril de 2011 - junho de 2012  (1 ano 3 meses)\n",
      "》Atuação no start-up da GVT no estado Rio de Janeiro, prestando suporte\n",
      "técnico e administrativo.\n",
      "ICATEL\n",
      "Auxiliar de Cadista\n",
      "janeiro de 2011 - abril de 2011  (4 meses)\n",
      "Rio de Janeiro e Região, Brasil\n",
      "》Prestação de suporte técnico e administrativo.\n",
      "Formação acadêmica\n",
      "Universidade Federal do Rio de Janeiro\n",
      "Master of Business Administration - MBA, Engenharia de\n",
      "Software  · (2020 - 2022)\n",
      "Universidade Candido Mendes\n",
      "Master of Business Administration - MBA, Economia e Gestão\n",
      "Empresarial  · (2019 - 2020)\n",
      "  Page 3 of 4\n",
      "   \n",
      "Senac RJ\n",
      "Pós Graduação, Design de Interação  · (2016 - 2018)\n",
      "Universidade Unigranrio | Afya\n",
      "Design Gráfico   · (2014 - 2016)\n",
      "Instituto Infnet\n",
      "Curso Superior de Tecnologia (CST), Data Science  · (janeiro de\n",
      "2023 - dezembro de 2025)\n",
      "  Page 4 of 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2 as pdf\n",
    "\n",
    "def import_pdf(pdf_path):\n",
    "    with open (pdf_path, 'rb') as file:\n",
    "        content = pdf.PdfReader(file)\n",
    "\n",
    "        text = ''\n",
    "\n",
    "        for page in content.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "        \n",
    "    return text\n",
    "\n",
    "pdf_path = 'data/linkedin_profile.pdf'\n",
    "\n",
    "text = import_pdf(pdf_path)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Exercício 2:\n",
    "Utilize Regex (módulo `re` nativo do Python) para criar uma função que, a partir do texto extraído, retorne um dicionário com as seguintes informações: \n",
    "* Seu número de telefone;\n",
    "* Seu endereço de email; e \n",
    "* O link do seu perfil no Linkedin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21) 98113-5927\n",
      "rodrigomesquita0@gmail.com\n",
      "www.linkedin.com/in/rodrigo-mesquita-6120b193\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def linkedin_reg(text):\n",
    "    telefone = re.search(r'\\([0-9]+\\)\\s[0-9]*-[0-9]*\\s+\\(Mobile\\)',text).group()\n",
    "    telefone = re.sub(r'\\s+\\(Mobile\\)', '', telefone)\n",
    "\n",
    "    email = re.search(r'\\w+(\\.)?\\w+@\\w+',text).group() + '.com'\n",
    "    \n",
    "    linkedin = re.search(r'www\\.linkedin\\.com\\S+\\n\\S+',text).group()\n",
    "    linkedin = linkedin.replace('\\n','')\n",
    "\n",
    "    print(telefone)\n",
    "    print(email)\n",
    "    print(linkedin)\n",
    "\n",
    "linkedin_reg(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 3:\n",
    "\n",
    "Aplique as funções geradas nas questões 1 e 2 para fazer o mesmo com o PDF em anexo (perfil do professor) e crie um CSV com as informações extraídas (colunas: nome, telefone, email e perfil) utilizando o módulo `csv` nativo do Python. Obs.: ao final os padrões utilizados no Regex devem abarcar os conteúdos dos dois PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51) 98128-0967\n",
      "viniciusbranco.ufrgs@gmail.com\n",
      "www.linkedin.com/in/vinicius-br-sc(LinkedIn)\n"
     ]
    }
   ],
   "source": [
    "pdf_path = 'data/linkedin_professor.pdf'\n",
    "\n",
    "text = import_pdf(pdf_path)\n",
    "linkedin_reg(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIs\n",
    "\n",
    "## Exercício 4:\n",
    "\n",
    "Explore o “playground” da API do SimilarWeb encontrada no RapidAPI (https://rapidapi.com/Glavier/api/similarweb12/playground/) e inscreva-se no plano gratuito, então crie um código para obter os dados dos 10 primeiros sites listados em “top-websites”, salvando-os em um dataframe do Pandas e enfim em um arquivo CSV usando o próprio Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://similarweb13.p.rapidapi.com/v2/getdomain-callback\"\n",
    "\n",
    "querystring = {\"domain\":\"google.com\",\"exclude-countries\":\"true\",\"country\": \"US\",\"limit\": 10 }\n",
    "\n",
    "\n",
    "headers = {\n",
    "\t\"x-rapidapi-key\": \"3d548dfb94msh2983fe233fc5869p15fdf5jsn4ff69403470e\",\n",
    "\t\"x-rapidapi-host\": \"similarweb13.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "print(response.json())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XPath\n",
    "\n",
    "## Exercício 5:\n",
    "\n",
    "Utilize o arquivo XML em anexo e a biblioteca `lxml` com caminhos relativos de XPath para:\n",
    "\n",
    "* Selecionar os nomes de todos *estudantes* que estejam no 2º ano ou acima dele;\n",
    "* Selecionar o nome do *professor* de Estruturas de Dados (course: \"Data Structures\");\n",
    "* Selecionar os títulos de todos os *cursos* ofertados pelo departamento de Ciência da Computação (department: Computer Science);\n",
    "* Selecionar os nomes de todos os *departamentos* que sejam pertencentes à Escola de Engenharia (college: Engineering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "\n",
    "tree = etree.parse(r'data/AT.xml', parser=etree.XMLParser())\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alice', 'Bob', 'Charlie', 'David', 'Grace', 'Henry', 'Isla', 'Jack']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Estudantes do 2° ano ou superior\n",
    "estudantes = root.xpath('//student[@year >= 2]/text()')\n",
    "estudantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr. Emily Clark']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nome do professor de Estruturas de Dados\n",
    "professores = root.xpath('//department/course[@code=\"CS202\"]/professor/text()')\n",
    "professores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Introduction to Computer Science', 'Data Structures']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cursos ofertados pelo departamento de Ciência da Computação\n",
    "cursos = root.xpath('//department[@name=\"Computer Science\"]/course/title/text()')\n",
    "cursos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Computer Science', 'Mechanical Engineering']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#departamentos pertencentes à Escola de Engenharia\n",
    "departamentos = root.xpath('//college[@name=\"Engineering\"]/department/@name')\n",
    "departamentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSS\n",
    "\n",
    "## Exercício 6:\n",
    "\n",
    "Utilize o arquivo XML em anexo e a biblioteca `lxml` com seletores de CSS para:\n",
    "\n",
    "Selecionar os títulos de todos os cursos cujos professores possuem estabilidade (tenure);\n",
    "Selecionar os títulos de todos os cursos que possuem horário de início pela manhã (AM). Dica: cuidado com nomes antigos de pseudo-classes, caso algum não funcione tente o nome antigo.\n",
    "Parte 5 WebCrawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício 7: \n",
    "\n",
    "Examine um site de sua escolha na lista de sites fornecida em anexo e descubra o padrão de URL para paginação que ele aceita. Então, utilize-o para obter uma lista de links de notícias requisitando as 2 primeiras páginas e raspando os links de cada uma através de um único seletor de CSS aplicado via `BeautifulSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScraping\n",
    "\n",
    "## Exercício 8:\n",
    "\n",
    "Faça um loop para os 3 primeiros links da lista obtida na questão anterior requisitando o HTML de cada página com a biblioteca que preferir (`urllib`, `requests`, etc.) e aplicando funções baseadas em `BeautifulSoup` para capturar e por fim salvar em um mesmo arquivo JSON, junto à URL de cada notícia e ao datetime do momento da requisição de cada página:\n",
    "\n",
    "* O objeto datetime (timezone-aware) da data e hora da publicação da notícia;\n",
    "* O título da notícia;\n",
    "* O corpo do texto da notícia;\n",
    "* O subtítulo da notícia (se houver);\n",
    "* O autor ou autores da notícia (se houver)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy\n",
    "\n",
    "## Exercício 9:\n",
    "\n",
    "Escolha um dos sites da lista fornecida (que não tenha sido escolhido nas anteriores) para montar um projeto no Scrapy que abarque tanto o Crawling quanto o Scraping, a fim de rodá-lo tal como na questão anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium\n",
    "\n",
    "## Exercício 10:\n",
    "Extraia uma lista de empregos do site https://br.indeed.com. Extraia os títulos dos empregos da primeira página de resultados ao pesquisar por \"Data Scientist\" na área da capital de seu estado. O site usa JavaScript para carregar as listas dinamicamente, o que significa que você não pode recuperar esses dados simplesmente usando solicitações ou BeautifulSoup. Escreva um script em Python usando Selenium para extrair os títulos dos empregos desta página junto a outras informações que você considere relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
